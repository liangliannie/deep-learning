
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Train your network &#8212; Badblock 1.0 documentation</title>
    <link rel="stylesheet" href="../_static/pyramid.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Visualize your results!" href="view.html" />
    <link rel="prev" title="Build your network!" href="quickstart.html" />
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Neuton&amp;subset=latin" type="text/css" media="screen" charset="utf-8" />
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Nobile:regular,italic,bold,bolditalic&amp;subset=latin" type="text/css" media="screen" charset="utf-8" />
<!--[if lte IE 6]>
<link rel="stylesheet" href="../_static/ie6.css" type="text/css" media="screen" charset="utf-8" />
<![endif]-->

  </head><body>

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="view.html" title="Visualize your results!"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="quickstart.html" title="Build your network!"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Badblock 1.0 documentation</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="train-your-network">
<h1>Train your network<a class="headerlink" href="#train-your-network" title="Permalink to this headline">¶</a></h1>
<p>In this part, we will focus on how to train our UNet. We will first cover several common loss functions and then go with how to train. Finally, we also give a method to further improve the training speed.</p>
<div class="section" id="loss-functions">
<h2>Loss functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h2>
<p>The loss function is the function to map an event or values of one or more to prepresent the cost associated with the event. And if we view the deep learning as an optimization problem, our goal will be to minimize the loss function.</p>
<p>To be simple, the loss function is a method to evaluate how well our neural network is and we use backward proprogation to change the weights in the network to further improve our neural network or say to minimize our cost/loss.</p>
<p>Here, I will enumerate several common used loss functions based on Pytorch.</p>
<div class="section" id="l1loss">
<h3>L1Loss<a class="headerlink" href="#l1loss" title="Permalink to this headline">¶</a></h3>
<p>L1Loss creates a criterion that measures the mean absolute error (MAE) between each element in the input x and target y.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">()</span>
</pre></div>
</div>
<div class="math notranslate nohighlight">
\[l(x,y) = L = \{l_1, ..., l_N\}^T, l_n = |x_n-y_n|\]</div>
<p>where N is the batch size.</p>
</div>
<div class="section" id="mseloss">
<h3>MSELoss<a class="headerlink" href="#mseloss" title="Permalink to this headline">¶</a></h3>
<p>MSELoss creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input x and target y.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
</pre></div>
</div>
<div class="math notranslate nohighlight">
\[l(x,y) = L = \{l_1, ..., l_N\}^T, l_n = (x_n-y_n)^2\]</div>
</div>
<div class="section" id="mssim">
<h3>MSSIM<a class="headerlink" href="#mssim" title="Permalink to this headline">¶</a></h3>
<p>MSSIM is the structural similarity(SSIM) index, which predicts the perceived quality of digital television and cinematic pictures, as well as other kinds of digital images and videos. To be simple, SSIM is used for measuring the similarity between two images, while it ranges in [0,1]. SSIM is designed to improve the traditional methods, while it works well while being combined with traditional methods, such as L1, L2, in deep learning.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">MSSSIM</span><span class="p">(</span><span class="n">window_size</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="combinations">
<h3>Combinations<a class="headerlink" href="#combinations" title="Permalink to this headline">¶</a></h3>
<p>Try a combination of different loss functions.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">Loss</span> <span class="o">=</span> <span class="n">L1Loss</span> <span class="o">+</span> <span class="n">MSSSIM</span>
<span class="n">Loss</span> <span class="o">=</span> <span class="n">L1Loss</span> <span class="o">+</span> <span class="n">MSELoss</span> <span class="o">+</span> <span class="n">MSSSIM</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Here, only three loss functions are listed, and there are still a lot available there online. Google it! You can even define you own loss function based on your task. Network is flexible, and so with the loss functions. DIY!</p>
</div>
</div>
</div>
<div class="section" id="train-train-train">
<h2>Train, Train, Train<a class="headerlink" href="#train-train-train" title="Permalink to this headline">¶</a></h2>
<p>We start with the optimizer and then the structure of training.</p>
<div class="section" id="optimizer">
<h3>Optimizer<a class="headerlink" href="#optimizer" title="Permalink to this headline">¶</a></h3>
<p>The optimizer is the package implementing optimization algorithms. Most commonly used methods are already supported in Pytorch, and we can easily construct an optimizer object in pytorch by using one line of code. Two examples of optimizers are listed below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">var1</span><span class="p">,</span> <span class="n">var2</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
</pre></div>
</div>
<p>where the parameters of the optimizer can be set, such as the learning rate, weight decay and so on when initiating the optimizer.</p>
<p>From my view, the optimizer takes job of the backward propagation and optimizes the neural network based on certain optimization algorithm. The details of pytorch optim can be found <a class="reference external" href="https://pytorch.org/docs/stable/optim.html">https://pytorch.org/docs/stable/optim.html</a> . To be simple, the optimizer change the weights in neural network to make it the optimal weights for mapping the input to target.</p>
</div>
<div class="section" id="backward-propagation">
<h3>Backward Propagation<a class="headerlink" href="#backward-propagation" title="Permalink to this headline">¶</a></h3>
<p>Training has been made really simple in Pytorch. As the example given in Pytorch, we only need to loop over the data iterator and feed the inputs into the neural network and optimize. The training is done!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>  <span class="c1"># loop over the dataset multiple times</span>

    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
        <span class="c1"># get the inputs</span>
        <span class="n">badsino</span><span class="p">,</span> <span class="n">goodsino</span> <span class="o">=</span> <span class="n">data</span>

        <span class="c1"># zero the parameter gradients</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># forward + backward + optimize</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">Unet</span><span class="p">(</span><span class="n">badsino</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">goodsino</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># print statistics</span>
        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2000</span> <span class="o">==</span> <span class="mi">1999</span><span class="p">:</span>    <span class="c1"># print every 2000 mini-batches</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">&#39;[</span><span class="si">%d</span><span class="s1">, </span><span class="si">%5d</span><span class="s1">] loss: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span>
                  <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">running_loss</span> <span class="o">/</span> <span class="mi">2000</span><span class="p">))</span>
            <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Finished Training&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>We can train our neural network in GPU just simply transfering the tensors and the neural network onto the GPU.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">network</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</pre></div>
</div>
<p class="last">This would do all the work! That’s amazing, right?</p>
</div>
</div>
</div>
<div class="section" id="warm-restart-optional">
<h2>Warm Restart?(Optional)<a class="headerlink" href="#warm-restart-optional" title="Permalink to this headline">¶</a></h2>
<p>The restart techniques are common in gradient-free optimizaion to deal with multimodal functions. This warm restarts borrows the idea from <a class="reference external" href="https://arxiv.org/abs/1608.03983">SGDR: Stochastic Gradient Descent with Warm Restarts</a>.</p>
<p>The idea of the warm restart is to simulate a new warm-started run/restart of the optimizer once for every certain epochs.</p>
<p>Try to restart your optimizer:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">next_reset</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Resetting Optimizer</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">opts</span><span class="o">.</span><span class="n">initial_lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">))</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">opts</span><span class="o">.</span><span class="n">lr_decay</span><span class="p">)</span>
    <span class="n">network</span><span class="o">.</span><span class="n">set_optimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>

    <span class="c1"># Set the next reset</span>
    <span class="n">next_reset</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">opts</span><span class="o">.</span><span class="n">warm_reset_length</span> <span class="o">+</span> <span class="n">warm_reset_increment</span>
    <span class="n">warm_reset_increment</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">opts</span><span class="o">.</span><span class="n">warm_reset_increment</span>
</pre></div>
</div>
<p>Specially, from the paper, we can do the warm restarts that are not performed from scratch but emulated by decaying the learning rate.</p>
<div class="math notranslate nohighlight">
\[lr = lr_{min} + \frac{1}{2} (lr_{max} - lr_{min}) (1 + cos(\frac{T_{cur}}{T_i}\pi))\]</div>
<p>where <span class="math notranslate nohighlight">\(lr_{min}\)</span> and <span class="math notranslate nohighlight">\(lr_{max}\)</span> are the ranges for the learning rate, and <span class="math notranslate nohighlight">\(T_{cur}\)</span> accounts for how many epochs have been performed since the last restart, and <span class="math notranslate nohighlight">\(T_{i}\)</span> is the index of epochs.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">DIY warm starts.</p>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Train your network</a><ul>
<li><a class="reference internal" href="#loss-functions">Loss functions</a><ul>
<li><a class="reference internal" href="#l1loss">L1Loss</a></li>
<li><a class="reference internal" href="#mseloss">MSELoss</a></li>
<li><a class="reference internal" href="#mssim">MSSIM</a></li>
<li><a class="reference internal" href="#combinations">Combinations</a></li>
</ul>
</li>
<li><a class="reference internal" href="#train-train-train">Train, Train, Train</a><ul>
<li><a class="reference internal" href="#optimizer">Optimizer</a></li>
<li><a class="reference internal" href="#backward-propagation">Backward Propagation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#warm-restart-optional">Warm Restart?(Optional)</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="quickstart.html"
                        title="previous chapter">Build your network!</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="view.html"
                        title="next chapter">Visualize your results!</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/usage/train.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="view.html" title="Visualize your results!"
             >next</a> |</li>
        <li class="right" >
          <a href="quickstart.html" title="Build your network!"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Badblock 1.0 documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2019, Liang.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.8.2.
    </div>
  </body>
</html>