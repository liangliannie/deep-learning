%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
\edef\sphinxdqmaybe{\ifdefined\DeclareUnicodeCharacterAsOptional\string"\fi}
  \DeclareUnicodeCharacter{\sphinxdqmaybe00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}
\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}
\addto\captionsenglish{\renewcommand{\tablename}{Table}}
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}
\addto\captionsenglish{\renewcommand{\sphinxnonalphabeticalgroupname}{Non-alphabetical}}
\addto\captionsenglish{\renewcommand{\sphinxsymbolsname}{Symbols}}
\addto\captionsenglish{\renewcommand{\sphinxnumbersname}{Numbers}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{1}



\title{Badblock Documentation}
\date{Mar 07, 2019}
\release{1.0}
\author{Liang Li}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\maketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{Installation!}
\label{\detokenize{usage/installation:installation}}\label{\detokenize{usage/installation::doc}}

\section{Install Python}
\label{\detokenize{usage/installation:install-python}}

\subsection{Install Python from \sphinxstyleemphasis{Python}}
\label{\detokenize{usage/installation:install-python-from-python}}
The latest Python can be downloaded and installed from \sphinxurl{https://www.python.org/downloads/} , a version above 3.6 is recommended.


\subsection{Install Python from \sphinxstyleemphasis{Conda}}
\label{\detokenize{usage/installation:install-python-from-conda}}
Conda can be found via \sphinxurl{https://docs.conda.io/projects/conda/en/latest/user-guide/install/}.

Conda is also recommended since many different versions of python could be changed based on:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+go}{source activate myenv}
\end{sphinxVerbatim}

with the whole list of pythons with different versions can be shown as:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+go}{conda env list}
\end{sphinxVerbatim}


\section{Install Pytorch}
\label{\detokenize{usage/installation:install-pytorch}}

\subsection{Before start with pytorch, make sure CUDA has been installed.}
\label{\detokenize{usage/installation:before-start-with-pytorch-make-sure-cuda-has-been-installed}}
Install your cuda from \sphinxurl{https://developer.nvidia.com/cuda-downloads?target\_os=Linux}.

\begin{sphinxadmonition}{note}{Note:}
Check here to update with the latest driver. A good match of driver with the GPU will largerly increase the speed, thus please make sure you have the latest driver with your GPU.
\end{sphinxadmonition}


\subsection{Install pytorch}
\label{\detokenize{usage/installation:id1}}
pytorch can be found via \sphinxurl{https://pytorch.org/get-started/locally/}.


\section{Other packages}
\label{\detokenize{usage/installation:other-packages}}
There are a list of other packages that are optional to install, while most could be install by using \sphinxstylestrong{pip}. {[}if you are under conda, make sure you are using the correct \sphinxstylestrong{pip} under the correct version of python{]}
\begin{itemize}
\item {} 
\$ pip install visdom

\item {} 
\$ pip install numpy

\item {} 
\$ pip install matplotlib

\item {} 
\$ pip install Pillow

\end{itemize}

Plus, please include ‘pytorch\_msssim’ folder if you want to use msssim as a loss, and if there are other package needed, try \sphinxstylestrong{pip}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+go}{pip install packagename}
\end{sphinxVerbatim}

NOW, you are all set with all the packages need for the deep learning Unet, and next we will forward to prepare our data.


\sphinxstrong{See also:}


If a python environment with pytorch is hard to obtain locally, Docker is always a good choice to make your network run in cloud. Note: a most recent pytorch with NVIDIA can be pulled from \sphinxurl{https://docs.nvidia.com/deeplearning/dgx/pytorch-release-notes/running.html}.




\chapter{Prepare your data!}
\label{\detokenize{usage/data:prepare-your-data}}\label{\detokenize{usage/data::doc}}

\section{Process the raw data.}
\label{\detokenize{usage/data:process-the-raw-data}}

\subsection{Read and reshape}
\label{\detokenize{usage/data:read-and-reshape}}
The size and the shape of the data would affect not only the output but also the learning speed of the network,
and hence making the raw data the shape we want and postprocessing it back to the origin shape are usually common
in machine learning.

In our case, we have .s file as both the good data and badblock data, which are too large for our network,
thus we need to reshape the data based on {[}TOF, Slice, W, L{]}. In simple cases without down/up sampling(we will cover this later), W and L do not matter, but there we need try to make W and L as a power of 2.

\sphinxhref{http://www.numpy.org/}{Numpy} provides the padding function:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pad}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{p}{(}\PYG{p}{(}\PYG{n}{x1}\PYG{p}{,} \PYG{n}{y1}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{n}{x2}\PYG{p}{,} \PYG{n}{y2}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{n}{x3}\PYG{p}{,} \PYG{n}{y3}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{n}{x4}\PYG{p}{,} \PYG{n}{y4}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wrap}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

In our case, if we have {[}TOF=34, Slice=815, W=50, L=520{]}, we can do:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pad}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wrap}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

to make it {[}TOF=34, Slice=815, W=64, L=520{]}, which would be good enough for 3 times downsampling since W and L can be divided by \(2^3\).

\begin{sphinxadmonition}{note}{Note:}
Here I only list one way to change the shape, and actually there might be tons of other methods which are good to try.
\end{sphinxadmonition}


\subsection{Save the data}
\label{\detokenize{usage/data:save-the-data}}
Here is something I want to talk about:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{my\PYGZus{}fn}\PYG{p}{(}\PYG{n}{foo}\PYG{p}{,} \PYG{n}{bar}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}A really useful function.}

\PYG{l+s+sd}{    Returns None}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
\end{sphinxVerbatim}
\phantomsection\label{\detokenize{usage/data:module-sino_process_tof}}\index{sino\_process\_tof (module)@\spxentry{sino\_process\_tof}\spxextra{module}}
This module is used to generate intermediate pickles before the training
Created on Thu Jun 14 16:19:07 2018
@author: bill
@update: liang
\index{Sinogram\_Processor (class in sino\_process\_tof)@\spxentry{Sinogram\_Processor}\spxextra{class in sino\_process\_tof}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/data:sino_process_tof.Sinogram_Processor}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{sino\_process\_tof.}}\sphinxbfcode{\sphinxupquote{Sinogram\_Processor}}}{\emph{data\_file}, \emph{for\_test=False}, \emph{slices=-1}, \emph{min\_blocks=0}, \emph{max\_blocks=0}}{}~\index{process\_data() (sino\_process\_tof.Sinogram\_Processor method)@\spxentry{process\_data()}\spxextra{sino\_process\_tof.Sinogram\_Processor method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/data:sino_process_tof.Sinogram_Processor.process_data}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{process\_data}}}{\emph{file}, \emph{tof}, \emph{slices}, \emph{theta}, \emph{dist}}{}
This function is used to process the sinogram file

The function will first read the file with the extension .s,
and find its corresponding good and badblock file, output a stack
with both good and bad sinograms: Note, here we padding the axis
to make it more suitable for the network to train, e.g. we make axis
3 from 50 to 64 by padding wrapped pixels from up and down
\begin{description}
\item[{Args:}] \leavevmode
file

\item[{Returns:}] \leavevmode
output\_sino

\item[{Raises:}] \leavevmode
IOError: An error occurred accessing the file .

\end{description}

\end{fulllineitems}

\index{process\_sino\_file() (sino\_process\_tof.Sinogram\_Processor method)@\spxentry{process\_sino\_file()}\spxextra{sino\_process\_tof.Sinogram\_Processor method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/data:sino_process_tof.Sinogram_Processor.process_sino_file}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{process\_sino\_file}}}{}{}
This function is used to process the sinogram file

The function will first read the file with the extension .s.hdr,
and find its corresponding good and badblock file, output a stack
with both good and bad sinograms
\begin{description}
\item[{Args:}] \leavevmode
self.file

\item[{Returns:}] \leavevmode
output\_sino\_xy

\item[{Raises:}] \leavevmode
IOError: An error occurred accessing the file .

\end{description}

\end{fulllineitems}

\index{remove\_block() (sino\_process\_tof.Sinogram\_Processor method)@\spxentry{remove\_block()}\spxextra{sino\_process\_tof.Sinogram\_Processor method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/data:sino_process_tof.Sinogram_Processor.remove_block}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{remove\_block}}}{\emph{orig\_img}, \emph{block\_num}}{}
This function is omitted.

\end{fulllineitems}


\end{fulllineitems}

\index{main() (in module sino\_process\_tof)@\spxentry{main()}\spxextra{in module sino\_process\_tof}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/data:sino_process_tof.main}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{sino\_process\_tof.}}\sphinxbfcode{\sphinxupquote{main}}}{}{}
The main function to call for processing all the data files.

\end{fulllineitems}

\index{process\_sinogram() (in module sino\_process\_tof)@\spxentry{process\_sinogram()}\spxextra{in module sino\_process\_tof}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/data:sino_process_tof.process_sinogram}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{sino\_process\_tof.}}\sphinxbfcode{\sphinxupquote{process\_sinogram}}}{\emph{file}}{}
This function is used to process one patient.

\end{fulllineitems}



\section{Loading all the data into the GPU is not only time consuming and not efficient.}
\label{\detokenize{usage/data:loading-all-the-data-into-the-gpu-is-not-only-time-consuming-and-not-efficient}}

\subsection{Load}
\label{\detokenize{usage/data:load}}

\chapter{Build your network!}
\label{\detokenize{usage/quickstart:module-unet}}\label{\detokenize{usage/quickstart:build-your-network}}\label{\detokenize{usage/quickstart::doc}}\index{unet (module)@\spxentry{unet}\spxextra{module}}
Created on Tue Mar 20 21:17:05 2018

@author: bill
@updated: Liang
\index{Sino\_repair\_net (class in unet)@\spxentry{Sino\_repair\_net}\spxextra{class in unet}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/quickstart:unet.Sino_repair_net}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{unet.}}\sphinxbfcode{\sphinxupquote{Sino\_repair\_net}}}{\emph{opts}, \emph{device\_ids}, \emph{load\_model=False}}{}~\index{optimize() (unet.Sino\_repair\_net method)@\spxentry{optimize()}\spxextra{unet.Sino\_repair\_net method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/quickstart:unet.Sino_repair_net.optimize}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{optimize}}}{\emph{output}, \emph{target\_img}, \emph{input\_img}}{}
\end{fulllineitems}

\index{save\_network() (unet.Sino\_repair\_net method)@\spxentry{save\_network()}\spxextra{unet.Sino\_repair\_net method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/quickstart:unet.Sino_repair_net.save_network}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{save\_network}}}{}{}
\end{fulllineitems}

\index{set\_optimizer() (unet.Sino\_repair\_net method)@\spxentry{set\_optimizer()}\spxextra{unet.Sino\_repair\_net method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/quickstart:unet.Sino_repair_net.set_optimizer}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{set\_optimizer}}}{\emph{optimizer}}{}
\end{fulllineitems}

\index{test() (unet.Sino\_repair\_net method)@\spxentry{test()}\spxextra{unet.Sino\_repair\_net method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/quickstart:unet.Sino_repair_net.test}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{test}}}{\emph{x}, \emph{y}}{}
\end{fulllineitems}

\index{train\_batch() (unet.Sino\_repair\_net method)@\spxentry{train\_batch()}\spxextra{unet.Sino\_repair\_net method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/quickstart:unet.Sino_repair_net.train_batch}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train\_batch}}}{\emph{input\_img}, \emph{target\_img}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{UNet (class in unet)@\spxentry{UNet}\spxextra{class in unet}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/quickstart:unet.UNet}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{unet.}}\sphinxbfcode{\sphinxupquote{UNet}}}{\emph{opts}}{}~\index{forward() (unet.UNet method)@\spxentry{forward()}\spxextra{unet.UNet method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/quickstart:unet.UNet.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{x}, \emph{test=False}}{}
Defines the computation performed at every call.

Should be overridden by all subclasses.

\begin{sphinxadmonition}{note}{Note:}
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}

\index{UNet\_down\_block (class in unet)@\spxentry{UNet\_down\_block}\spxextra{class in unet}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/quickstart:unet.UNet_down_block}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{unet.}}\sphinxbfcode{\sphinxupquote{UNet\_down\_block}}}{\emph{input\_channel}, \emph{output\_channel}, \emph{down\_sample}}{}~\index{forward() (unet.UNet\_down\_block method)@\spxentry{forward()}\spxextra{unet.UNet\_down\_block method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/quickstart:unet.UNet_down_block.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{x}}{}
Defines the computation performed at every call.

Should be overridden by all subclasses.

\begin{sphinxadmonition}{note}{Note:}
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}

\index{UNet\_up\_block (class in unet)@\spxentry{UNet\_up\_block}\spxextra{class in unet}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/quickstart:unet.UNet_up_block}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{unet.}}\sphinxbfcode{\sphinxupquote{UNet\_up\_block}}}{\emph{prev\_channel}, \emph{input\_channel}, \emph{output\_channel}, \emph{ID}}{}~\index{forward() (unet.UNet\_up\_block method)@\spxentry{forward()}\spxextra{unet.UNet\_up\_block method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/quickstart:unet.UNet_up_block.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{prev\_feature\_map}, \emph{x}}{}
Defines the computation performed at every call.

Should be overridden by all subclasses.

\begin{sphinxadmonition}{note}{Note:}
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}



\chapter{Train your network!}
\label{\detokenize{usage/train:module-train_unet}}\label{\detokenize{usage/train:train-your-network}}\label{\detokenize{usage/train::doc}}\index{train\_unet (module)@\spxentry{train\_unet}\spxextra{module}}
Created on Thu Jan 15 18:45:25 2019

@author: bill
@updated: Liang
\index{Network\_Trainer (class in train\_unet)@\spxentry{Network\_Trainer}\spxextra{class in train\_unet}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/train:train_unet.Network_Trainer}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{train\_unet.}}\sphinxbfcode{\sphinxupquote{Network\_Trainer}}}{\emph{opts}, \emph{device\_ids}}{}~\index{test() (train\_unet.Network\_Trainer method)@\spxentry{test()}\spxextra{train\_unet.Network\_Trainer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/train:train_unet.Network_Trainer.test}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{test}}}{\emph{network}, \emph{batch\_size}, \emph{perform\_recon\_loss=False}, \emph{output\_all\_sinos=False}}{}
\end{fulllineitems}

\index{train() (train\_unet.Network\_Trainer method)@\spxentry{train()}\spxextra{train\_unet.Network\_Trainer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/train:train_unet.Network_Trainer.train}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{}{}
\end{fulllineitems}

\index{visualize\_progress() (train\_unet.Network\_Trainer method)@\spxentry{visualize\_progress()}\spxextra{train\_unet.Network\_Trainer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/train:train_unet.Network_Trainer.visualize_progress}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{visualize\_progress}}}{\emph{opts}, \emph{images}, \emph{istest=False}}{}
\end{fulllineitems}

\index{write\_log() (train\_unet.Network\_Trainer method)@\spxentry{write\_log()}\spxextra{train\_unet.Network\_Trainer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/train:train_unet.Network_Trainer.write_log}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{write\_log}}}{\emph{value}, \emph{log\_type='test'}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{fill\_image() (in module train\_unet)@\spxentry{fill\_image()}\spxextra{in module train\_unet}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/train:train_unet.fill_image}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{train\_unet.}}\sphinxbfcode{\sphinxupquote{fill\_image}}}{\emph{input\_sino}, \emph{out\_img}}{}
\end{fulllineitems}

\index{main() (in module train\_unet)@\spxentry{main()}\spxextra{in module train\_unet}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/train:train_unet.main}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{train\_unet.}}\sphinxbfcode{\sphinxupquote{main}}}{}{}
\end{fulllineitems}

\index{preprocess() (in module train\_unet)@\spxentry{preprocess()}\spxextra{in module train\_unet}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/train:train_unet.preprocess}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{train\_unet.}}\sphinxbfcode{\sphinxupquote{preprocess}}}{\emph{input\_img}, \emph{target\_img}}{}
\end{fulllineitems}

\index{random() (in module train\_unet)@\spxentry{random()}\spxextra{in module train\_unet}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{usage/train:train_unet.random}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{train\_unet.}}\sphinxbfcode{\sphinxupquote{random}}}{}{{ $\rightarrow$ x in the interval {[}0, 1).}}
\end{fulllineitems}



\chapter{Visualize your results!}
\label{\detokenize{usage/view:visualize-your-results}}\label{\detokenize{usage/view::doc}}

\chapter{Adjust your parameters!}
\label{\detokenize{usage/adjust:adjust-your-parameters}}\label{\detokenize{usage/adjust::doc}}
To start to view the result by using visdom:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+go}{python \PYGZhy{}m visdom.server}
\end{sphinxVerbatim}

To watch the using of GPU by using:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+go}{watch \PYGZhy{}n 1 nvidia\PYGZhy{}smi}
\end{sphinxVerbatim}


\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{s}
\item\relax\sphinxstyleindexentry{sino\_process\_tof}\sphinxstyleindexpageref{usage/data:\detokenize{module-sino_process_tof}}
\indexspace
\bigletter{t}
\item\relax\sphinxstyleindexentry{train\_unet}\sphinxstyleindexpageref{usage/train:\detokenize{module-train_unet}}
\indexspace
\bigletter{u}
\item\relax\sphinxstyleindexentry{unet}\sphinxstyleindexpageref{usage/quickstart:\detokenize{module-unet}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}